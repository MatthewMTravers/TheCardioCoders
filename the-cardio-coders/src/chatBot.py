from langchain_ollama.llms import OllamaLLM
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import JSONLoader
import json
from langchain.schema import Document
from uuid import uuid4
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer


# Initialize the LLM and Embeddings
ollama = OllamaLLM(model='llama3.2')
#initializing the embedding model which is the same as used in jupyter notebook
embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')


#loading the FAISS index and document embeddings
index = faiss.read_index('faiss_index.index')
embeddings_matrix = np.load('embeddings.npy')


#loading documents
raw_json_docs = ["stretches.json", "exercises1.json", "exercises2.json"]
documents = []
num = 0

# Process JSON files
for file in raw_json_docs:
    #loading the document
    json_loader = JSONLoader(file, jq_schema=".", text_content=False)
    doc = json_loader.load()    

    #splitting the document
    for exercise in doc:

        #getting the page content from json, ignoring metadata
        exercise_content = exercise.page_content

        #convert json string to list of dictionaries
        exercise_list = json.loads(exercise_content)

        #add each exercise as an individual document
        for exercise_item in exercise_list:
            document = Document(page_content=str(exercise_item), metadata={"source": file, "seq_num": num+1})
            documents.append(document)



# Function to query the FAISS vector store and get relevant documents
def query_vector_store(query, k=5):
    # Convert the query into an embedding
    query_embedding = embedding_model.encode([query])[0]
    
    # Perform a similarity search on the FAISS index
    query_embedding = np.array([query_embedding]).astype(np.float32)
    D, I = index.search(query_embedding, k)  # Retrieve the top k most similar documents
    
    # Fetch the corresponding documents based on the indices returned by FAISS
    relevant_docs = [documents[i] for i in I[0]]
    
    return relevant_docs


from langchain import hub
from typing_extensions import List, TypedDict
from langchain_core.documents import Document
from langgraph.graph import START, StateGraph

prompt = hub.pull("rlm/rag-prompt")

class State(TypedDict):
    question: str
    context: List[Document]
    answer: str

#define application steps
def retrieve(state:State):
    retrieved_docs = query_vector_store(state["question"])
    print("Retrieved Docs:", [doc.page_content for doc in retrieved_docs])  # Debugging
    return {"context": retrieved_docs}


def generate(state: State):
    docs_content = "\n\n".join([doc.page_content for doc in state["context"]])
    messages = prompt.invoke({"question": state["question"], "context":docs_content})
    response = ollama.invoke(messages)
    return {"answer": response}


#compile application and test. uses langgraph to make a state graph
graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()


response = graph.invoke({"question": "what are soome good stretches for my back"})
print(response["answer"])


# # Query input
# query = "What are some good stretches for my back?"

# # Retrieve relevant documents from the FAISS index
# relevant_docs = query_vector_store(query)

# # Combine the relevant documents into a context string for the LLM
# context = "\n".join([doc.page_content for doc in relevant_docs])

# # Use Ollama as the LLM to generate a response based on the context
# response = ollama.invoke(messages=[{"role": "user", "content": f"Context:\n{context}\n\nQuestion: {query}"}])

# # Print the response generated by Ollama
# print(response["text"])

